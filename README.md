# Transformer-PPO

**Transformer-PPO** integrates the Transformer architecture with Proximal Policy Optimization (PPO) to enhance reinforcement learning (RL) performance. By leveraging the Transformer's attention mechanisms, this project aims to improve policy learning in complex environments.

---

## üöÄ Features

- **Transformer-Based Policy Network**: Utilizes the Transformer's capacity to model sequential data, enhancing decision-making processes.
- **Stable Training with PPO**: Employs PPO, a reliable RL algorithm, ensuring stable and efficient policy updates.
- **Customizable Environments**: Supports integration with various RL environments for diverse experimentation.

---

## üìÇ Installation

### 1Ô∏è‚É£ Clone the repository:

```bash
git clone https://github.com/mtr26/Transformer-PPO.git
cd Transformer-PPO
```

## 2Ô∏è‚É£ Create a virtual environment (optional but recommended):
```bash
python3 -m venv env
source env/bin/activate  # On Windows, use 'env\Scripts\activate'
```

## 3Ô∏è‚É£ Install dependencies:
```bash
pip install -r requirements.txt
```

## üìÑ License
This project is licensed under the MIT License.

## ‚≠ê Acknowledgments
CleanRL's PPO with Transformer-XL: Inspiration for integrating Transformer architectures with PPO.
Hugging Face's TRL: Tools and libraries for training transformer models with reinforcement learning.
